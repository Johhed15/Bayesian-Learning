---
title: "Computer lab 3"
date: "`r Sys.Date()`"
author: "Johannes Hedström & Mikael Montén "
output:
  pdf_document:
    latex_engine: xelatex
    fig_caption: yes
    number_sections: yes
  html_document:
    df_print: paged
geometry: top=100pt,bottom=100pt,left=68pt,right=66pt
header-includes:
- \usepackage{float}
- \usepackage{longtable}
- \usepackage{caption}
- \usepackage{fancyhdr}
- \usepackage{titling}
- \renewcommand{\headrulewidth}{0pt}
- \renewcommand{\and}{\\}
- \pretitle{\centering\vspace{0cm}{732A73 Bayesian Learning \par}\vspace{5cm}\Huge\textbf}
- \posttitle{\vspace{1cm}\large\textbf{}\par}
- \preauthor{\centering\vspace{4cm}\normalsize}
- \postauthor{\par\vspace{2cm}}
- \predate{\centering{\normalsize STIMA \\
  Department of Computer and Information Science \\ Linköpings universitet \par}}
- \postdate{\par\vspace{0cm}}
- \raggedbottom
---


<!-- page number pos -->
\fancyhf{}
\fancyfoot[C]{\thepage}
\pagestyle{fancy}

<!-- no page nr on first page  -->
\pagenumbering{gobble}

<!-- Anger sidbrytning -->
\clearpage

<!-- creating the table of contents -->
\setcounter{tocdepth}{3}
\tableofcontents

<!-- new page -->
\clearpage

<!-- starting the count on 1 after the contents -->
\pagenumbering{arabic}
\setcounter{page}{1}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning=FALSE, fig.width = 5, fig.height = 3, fig.align = 'center')
set.seed(12345)
```

```{r}
library(coda)
library(readxl)
library(mvtnorm)
library(tidyr)
library(BayesLogit)
library(tinytex)
```

# Gibbs sampling for the logistic regression

Consider again the logistic regression model in problem 2 from the previous computer lab 2. Use the prior $\beta \sim N (0,\tau^2)$ where $\tau = 3$


## a 

Implement (code!) a Gibbs sampler that simulates from the joint posterior $p(w,\beta|x)$ by augmenting the data with Polya-gamma latent variables $w_i, i = 1 \ldots n$. The full conditional posteriors are given on the slides from Lecture 7. Evaluate the convergence of the gibbs sampler by calculating the Inefficienty Factors(IFs) and by plotting the trajectories of the samples Markov chains.



```{r}
women <- read.table('WomenAtWork.dat', header=TRUE)

# picking out the variables from the data
y <- women$Work
X <- as.matrix(women[,-1])

# creating variables that are used in the sampling
tau <- 3
betas <- rep(1,7)
n_samp <- 1000
kappa <- y-0.5
beta_samples <- matrix(NA, nrow = n_samp, ncol = 7)
beta_samples[1,] <- betas

wi <- matrix(0,nrow = ncol(X),ncol=n_samp)
for (i in 2:n_samp){
  
  for (j in 1:nrow(X)){

    wi[j,i-1] <- rpg(1,X[j,]%*%beta_samples[i-1,])
  }
  omega <- diag(wi[i-1])
  B_prior <- dmvnorm(betas,rep(1,7),diag(tau ^ 2,7), log=TRUE)
  
  Vw <- solve(t(X)%*%omega%*%X + solve(diag(tau ^ 2,7))) # equation for Vm
  mw <- vm %*%(t(X)*k) # equatino for mw
  beta_samples[i,] <- dmvnorm(1,mw,Vm)
  #loglik <- sum( linpred*y - log(1 + exp(linpred)))

  #B_prior+loglik
  
}




```



## b
Use the posterior draws from a) to compute 90% equal tail credible interval for $Pr(y=1|x)$ where x corresponds to a 38-year-old woman with 1 chils(3 years old), 12 years of education and 7 years of experience and a husband with an income of 22. A 90% equal tail credible interval (a,b) cuts the off 5% percent of the posterior probability mass to the laft of a, and 5% to the right of b. 

```{r}

```



# Metropolis Random Walk for Poisson regression

Consider the following Poisson regression model

$$y_i |\beta \overset{iid}\sim Poisson [exp(x_i^T \beta)]$$

where yi is the count for the ith observation in the sample and xi is the p-dimensional
vector with covariate observations for the ith observation. Use the data set eBayNumberOfBidderData_2024.dat. This dataset contains observations from 800 eBay auctions of coins. The response variable is nBids and records the number of
bids in each auction. The remaining variables are features/covariates (x):


* Const (for the intercept)

* PowerSeller (equal to 1 if the seller is selling large volumes on eBay)

* VerifyID (equal to 1 if the seller is a verifed seller by eBay)

* Sealed (equal to 1 if the coin was sold in an unopened envelope)

* MinBlem (equal to 1 if the coin has a minor defect)

* MajBlem (equal to 1 if the coin has a major defect)

* LargNeg (equal to 1 if the seller received a lot of negative feedback from
customers)

* LogBook (logarithm of the book value of the auctioned coin according to
expert sellers. Standardized)

* MinBidShare (ratio of the minimum selling price (starting price) to the book
value. Standardized).


## a)

```{r}
women <- read.table('eBayNumberOfBidderData_2024.dat', header=TRUE)

```


Obtain the maximum likelihood estimator of $\beta$ in the Poisson regression model
for the eBay data [Hint: glm.R, don't forget that glm() adds its own intercept
so don't input the covariate Const]. Which covariates are signifcant?


## b)

Let's do a Bayesian analysis of the Poisson regression. Let the prior be $\beta \sim N[0,100\cdot(X^TX)^{-1}]$ , where X is the n x p covariate matrix. This is a commonly used prior, which is called Zellner's g-prior. Assume first that the posterior density is approximately multivariate normal:

$$\beta>y \sim N(\hat\beta, J_y^{-1}(\hat\beta))$$

where $\hat\beta$ is the posterior mode and $J_y(\hat\beta)$ is the negative hessian at the posterior mode.$\hat\beta$ and $J_y(\hat\beta)$ can be obtained by numerical optimization(optim.R) exactly like youve already did for the first logistic regression in Lab 2 (but with the log posterior function replaced by the corresponding one for the Poisson model, which you have code up.).



## c) 

Let's simulate from the actual posterior of $\beta$ using the Metropolis algorithm
and compare the results with the approximate results in b). Program a general
function that uses the Metropolis algorithm to generate random draws from an
arbitrary posterior density. In order to show that it is a general function for
any model, we denote the vector of model parameters by $\theta$. Let the proposal
density be the multivariate normal density mentioned in Lecture 8 (random
walk Metropolis):

$$\theta_p|\theta^{(i-1)} \sim N(\theta^{(i-1)}, c \cdot \Sigma)$$


$\Sigma = J_y^{-1}(\hat\beta)$ was obtained in b). The value c is a tuning parameter and
should be an input to your Metropolis function. The user of your Metropolis function should be able to supply her own posterior density function, not
necessarily for the Poisson regression, and still be able to use your Metropolis
function. This is not so straightforward, unless you have come across function objects in R. The note HowToCodeRWM.pdf in Lisam describes how you can do this in R.

Now, use your new Metropolis function to sample from the posterior of $\beta$
in the Poisson regression for the eBay dataset. Assess MCMC convergence by
graphical methods

## d)

Use the MCMC draws from c) to simulate from the predictive distribution of
the number of bidders in a new auction with the characteristics below. Plot
the predictive distribution. What is the probability of no bidders in this new
auction?

* PowerSeller = 1
* VerifyID = 0
* Sealed = 1
* MinBlem = 0
* MajBlem = 1
* LargNeg = 0
* LogBook = 1.2
* MinBidShare = 0.8



# Time series models in Stan

## a) 

Write a function in R that simulates data from the AR(1)-process

$$ x _t = \mu + \phi (x_{t-1}- \mu) + \epsilon_t, \epsilon_t \overset{iid}\sim N(0,\sigma^2)$$


for given values of $\mu, \phi \text{ and } \sigma^"$. Start the process at x1 = $\mu$ and then simulate
values for xt for t = 2,3...T and return the vector x1:T containing all time
points. Use $\mu$ = 9, $\sigma$ = 4 and T = 250 and look at some different realizations
(simulations) of x1:T for values of $\phi$ between −1 and 1 (this is the interval
of  $\phi$  where the AR(1)-process is stationary). Include a plot of at least one
realization in the report. What effect does the value of  $\phi$  have on $x_{1:T}$ 


```{r}

mu <- 9
sigma <- 4
phi <- seq(-1,1,by=0.1)
xmat <- matrix(mu,ncol=length(phi), nrow=250)

for (j in 1:length(phi)){# looping over different phi values
  for (i in 2:250){ # looping over all time stamps
  xmat[i,j] <- mu + phi[j]*(xmat[(i-1),j] - mu) + rnorm(1,0,sigma)   
}
}

plot(x=c(1:250), y=xmat[,1],type='l')
lines(xmat[,21], col='red')
```

The value of phi makes how xt should depend on the previous value, positive/negative and how much it should depend on it. 

## b)

Use your function from a) to simulate two AR(1)-processes, x1:T with $\phi$ = 0.3
and y1:T with$\phi$ = 0.97. Now, treat your simulated vectors as synthetic data,
and treat the values of $\mu$, $\phi$ and $\sigma^2$ as unknown parameters. Implement Stancode that samples from the posterior of the three parameters, using suitable
non-informative priors of your choice. [Hint: Look at the time-series models
examples in the Stan user's guide/reference manual, and note the different
parameterization used here.]

* Report the posterior mean, 95% credible intervals and the number of effective posterior samples for the three inferred parameters for each of thesimulated AR(1)-process. Are you able to estimate the true values?

* For each of the two data sets, evaluate the convergence of the samplers and plot the joint posterior of $\mu$ and $\phi$. Comments?


```{r}
mu <- 9
sigma <- 4
phi2 <- c(0.3,0.97)
xmat <- matrix(mu,ncol=2, nrow=250)

for (j in 1:length(phi2)){# looping over different phi values
  for (i in 2:250){ # looping over all time stamps
  xmat[i,j] <- mu + phi2[j]*(xmat[(i-1),j] - mu) + rnorm(1,0,sigma)   
}
}

plot(x=c(1:250), y=xmat[,2],type='l')
lines(xmat[,1], col='red')
```



